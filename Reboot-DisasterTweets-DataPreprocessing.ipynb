{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impacting the performance by 2 percent why? -- see with \n",
    "def preprocess_tweets(tweets):\n",
    "    '''\n",
    "    Convert to lowercase\n",
    "    Remove http tag and urls\n",
    "    Remove users\n",
    "    Remove non alpha numeric terms. \n",
    "    '''\n",
    "    tweets['text'] = tweets['text'].str.lower()\n",
    "    tweets['text'] = tweets['text'].str.replace(r\"http\\S+\", \"\")\n",
    "    tweets['text'] = tweets['text'].str.replace(r\"http\", \"\")\n",
    "    tweets['text'] = tweets['text'].str.replace(r\"https?://\\S+|www\\.\\S+\",\"\")\n",
    "    tweets['text'] = tweets['text'].str.replace(r\"@\\S+\", \"USER\")\n",
    "    #tweets['text'] = tweets['text'].str.replace(r\"@\", \"at\")\n",
    "    tweets['text'] = tweets['text'].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    return tweets\n",
    "                                                \n",
    "def remove_html(text):\n",
    "    html = re.compile('<.*?>')\n",
    "    return html.sub('',text)\n",
    "                                                \n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "def apply_stemming(tweets):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tweets['tokens'] = tweets['text'].apply(wordpunct_tokenize)\n",
    "    stemmer = PorterStemmer()\n",
    "    tweets['tokens'] = tweets['tokens'].apply(lambda word:[stemmer.stem(w) for w in word])\n",
    "    tweets['text'] = tweets[\"tokens\"].str.join(\" \")\n",
    "    tweets.drop(labels = ['tokens'], axis = 1, inplace = True)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the input training file and create the data frame\n",
    "tweets = pd.read_csv('../Data/train.csv', header = 'infer', delimiter = ',')\n",
    "tweets.drop(labels = ['id', 'keyword', 'location'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform text cleaning and preprocessing. \n",
    "tweets = preprocess_tweets(tweets)\n",
    "tweets['text'] = tweets['text'].apply(lambda x: remove_html(x))\n",
    "tweets['text'] = tweets['text'].apply(lambda x: remove_emoji(x))\n",
    "tweets['text'] = tweets['text'].apply(lambda x: remove_punct(x))\n",
    "tweets = apply_stemming(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>our deed are the reason of thi earthquak may a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all resid ask to shelter in place are be notif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13000 peopl receiv wildfir evacu order in cali...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just got sent thi photo from rubi alaska as sm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>two giant crane hold a bridg collaps into near...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>user user the out of control wild fire in cali...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>m1 94 01 04 utc 5km s of volcano hawaii</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>polic investig after an e bike collid with a c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>the latest more home raze by northern californ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "0     our deed are the reason of thi earthquak may a...       1\n",
       "1                  forest fire near la rong sask canada       1\n",
       "2     all resid ask to shelter in place are be notif...       1\n",
       "3     13000 peopl receiv wildfir evacu order in cali...       1\n",
       "4     just got sent thi photo from rubi alaska as sm...       1\n",
       "...                                                 ...     ...\n",
       "7608  two giant crane hold a bridg collaps into near...       1\n",
       "7609  user user the out of control wild fire in cali...       1\n",
       "7610            m1 94 01 04 utc 5km s of volcano hawaii       1\n",
       "7611  polic investig after an e bike collid with a c...       1\n",
       "7612  the latest more home raze by northern californ...       1\n",
       "\n",
       "[7613 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('../Data/clean_train.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Process test set\n",
    "tweets_test = pd.read_csv('../Data/test.csv', header = 'infer', delimiter = ',')\n",
    "tweets_test.drop(labels = ['id', 'keyword', 'location'], axis = 1, inplace = True)\n",
    "\n",
    "tweets_test = preprocess_tweets(tweets_test)\n",
    "tweets_test['text'] = tweets_test['text'].apply(lambda x: remove_html(x))\n",
    "tweets_test['text'] = tweets_test['text'].apply(lambda x: remove_emoji(x))\n",
    "tweets_test['text'] = tweets_test['text'].apply(lambda x: remove_punct(x))\n",
    "twetweets_testets = apply_stemming(tweets_test)\n",
    "\n",
    "tweets_test.to_csv('../Data/clean_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
